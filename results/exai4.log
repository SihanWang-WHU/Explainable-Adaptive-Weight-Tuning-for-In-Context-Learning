/usr/bin/python3 /root/workdir/squad.py
Loading dataset...
Creating DataLoaders...
Initializing model...
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training...
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)
Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased-finetuned-qa-mlqa and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Layer-Head Absolute Differences (Averaged):
bert.embeddings.word_embeddings.weight: 0.715
bert.embeddings.position_embeddings.weight: 0.248
bert.embeddings.token_type_embeddings.weight: 0.179
bert.embeddings.LayerNorm.weight: 1.734
bert.embeddings.LayerNorm.bias: 0.831
bert.encoder.layer.0.head.0: 3.017
bert.encoder.layer.0.head.1: 1.771
bert.encoder.layer.0.head.2: 2.315
bert.encoder.layer.0.head.3: 3.551
bert.encoder.layer.0.head.4: 1.683
bert.encoder.layer.0.head.5: 3.959
bert.encoder.layer.0.head.6: 1.972
bert.encoder.layer.0.head.7: 1.762
bert.encoder.layer.0.head.8: 1.662
bert.encoder.layer.0.head.9: 1.949
bert.encoder.layer.0.head.10: 2.344
bert.encoder.layer.0.head.11: 2.167
bert.encoder.layer.0.attention.output.dense.weight: 0.141
bert.encoder.layer.0.attention.output.dense.bias: 0.283
bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.173
bert.encoder.layer.0.attention.output.LayerNorm.bias: 1.238
bert.encoder.layer.0.intermediate.dense.weight: 0.185
bert.encoder.layer.0.intermediate.dense.bias: 0.243
bert.encoder.layer.0.output.dense.weight: 0.175
bert.encoder.layer.0.output.dense.bias: 0.351
bert.encoder.layer.0.output.LayerNorm.weight: 0.693
bert.encoder.layer.0.output.LayerNorm.bias: 0.362
bert.encoder.layer.1.head.0: 2.344
bert.encoder.layer.1.head.1: 1.640
bert.encoder.layer.1.head.2: 0.989
bert.encoder.layer.1.head.3: 1.078
bert.encoder.layer.1.head.4: 1.686
bert.encoder.layer.1.head.5: 1.136
bert.encoder.layer.1.head.6: 1.486
bert.encoder.layer.1.head.7: 2.081
bert.encoder.layer.1.head.8: 1.266
bert.encoder.layer.1.head.9: 1.936
bert.encoder.layer.1.head.10: 1.646
bert.encoder.layer.1.head.11: 1.114
bert.encoder.layer.1.attention.output.dense.weight: 0.139
bert.encoder.layer.1.attention.output.dense.bias: 0.304
bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.387
bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.761
bert.encoder.layer.1.intermediate.dense.weight: 0.195
bert.encoder.layer.1.intermediate.dense.bias: 0.235
bert.encoder.layer.1.output.dense.weight: 0.185
bert.encoder.layer.1.output.dense.bias: 0.306
bert.encoder.layer.1.output.LayerNorm.weight: 0.416
bert.encoder.layer.1.output.LayerNorm.bias: 0.412
bert.encoder.layer.2.head.0: 1.992
bert.encoder.layer.2.head.1: 1.369
bert.encoder.layer.2.head.2: 1.488
bert.encoder.layer.2.head.3: 1.452
bert.encoder.layer.2.head.4: 1.403
bert.encoder.layer.2.head.5: 1.056
bert.encoder.layer.2.head.6: 1.432
bert.encoder.layer.2.head.7: 0.937
bert.encoder.layer.2.head.8: 1.133
bert.encoder.layer.2.head.9: 2.145
bert.encoder.layer.2.head.10: 1.165
bert.encoder.layer.2.head.11: 1.437
bert.encoder.layer.2.attention.output.dense.weight: 0.136
bert.encoder.layer.2.attention.output.dense.bias: 0.397
bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.398
bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.705
bert.encoder.layer.2.intermediate.dense.weight: 0.197
bert.encoder.layer.2.intermediate.dense.bias: 0.270
bert.encoder.layer.2.output.dense.weight: 0.186
bert.encoder.layer.2.output.dense.bias: 0.321
bert.encoder.layer.2.output.LayerNorm.weight: 0.331
bert.encoder.layer.2.output.LayerNorm.bias: 0.382
bert.encoder.layer.3.head.0: 1.656
bert.encoder.layer.3.head.1: 1.006
bert.encoder.layer.3.head.2: 1.355
bert.encoder.layer.3.head.3: 1.056
bert.encoder.layer.3.head.4: 1.155
bert.encoder.layer.3.head.5: 1.314
bert.encoder.layer.3.head.6: 1.241
bert.encoder.layer.3.head.7: 1.927
bert.encoder.layer.3.head.8: 1.182
bert.encoder.layer.3.head.9: 1.362
bert.encoder.layer.3.head.10: 1.318
bert.encoder.layer.3.head.11: 1.035
bert.encoder.layer.3.attention.output.dense.weight: 0.149
bert.encoder.layer.3.attention.output.dense.bias: 0.277
bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.290
bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.703
bert.encoder.layer.3.intermediate.dense.weight: 0.199
bert.encoder.layer.3.intermediate.dense.bias: 0.280
bert.encoder.layer.3.output.dense.weight: 0.190
bert.encoder.layer.3.output.dense.bias: 0.327
bert.encoder.layer.3.output.LayerNorm.weight: 0.569
bert.encoder.layer.3.output.LayerNorm.bias: 0.318
bert.encoder.layer.4.head.0: 1.224
bert.encoder.layer.4.head.1: 1.074
bert.encoder.layer.4.head.2: 1.740
bert.encoder.layer.4.head.3: 1.589
bert.encoder.layer.4.head.4: 1.268
bert.encoder.layer.4.head.5: 1.236
bert.encoder.layer.4.head.6: 1.307
bert.encoder.layer.4.head.7: 1.654
bert.encoder.layer.4.head.8: 1.162
bert.encoder.layer.4.head.9: 0.978
bert.encoder.layer.4.head.10: 1.060
bert.encoder.layer.4.head.11: 1.581
bert.encoder.layer.4.attention.output.dense.weight: 0.159
bert.encoder.layer.4.attention.output.dense.bias: 0.207
bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.326
bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.608
bert.encoder.layer.4.intermediate.dense.weight: 0.198
bert.encoder.layer.4.intermediate.dense.bias: 0.322
bert.encoder.layer.4.output.dense.weight: 0.190
bert.encoder.layer.4.output.dense.bias: 0.305
bert.encoder.layer.4.output.LayerNorm.weight: 0.539
bert.encoder.layer.4.output.LayerNorm.bias: 0.293
bert.encoder.layer.5.head.0: 1.223
bert.encoder.layer.5.head.1: 1.728
bert.encoder.layer.5.head.2: 1.282
bert.encoder.layer.5.head.3: 1.196
bert.encoder.layer.5.head.4: 1.032
bert.encoder.layer.5.head.5: 1.121
bert.encoder.layer.5.head.6: 1.368
bert.encoder.layer.5.head.7: 1.344
bert.encoder.layer.5.head.8: 1.384
bert.encoder.layer.5.head.9: 1.309
bert.encoder.layer.5.head.10: 1.158
bert.encoder.layer.5.head.11: 1.099
bert.encoder.layer.5.attention.output.dense.weight: 0.159
bert.encoder.layer.5.attention.output.dense.bias: 0.230
bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.289
bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.569
bert.encoder.layer.5.intermediate.dense.weight: 0.196
bert.encoder.layer.5.intermediate.dense.bias: 0.341
bert.encoder.layer.5.output.dense.weight: 0.187
bert.encoder.layer.5.output.dense.bias: 0.279
bert.encoder.layer.5.output.LayerNorm.weight: 0.823
bert.encoder.layer.5.output.LayerNorm.bias: 0.282
bert.encoder.layer.6.head.0: 1.597
bert.encoder.layer.6.head.1: 1.097
bert.encoder.layer.6.head.2: 1.355
bert.encoder.layer.6.head.3: 1.507
bert.encoder.layer.6.head.4: 1.459
bert.encoder.layer.6.head.5: 1.234
bert.encoder.layer.6.head.6: 1.338
bert.encoder.layer.6.head.7: 1.305
bert.encoder.layer.6.head.8: 1.442
bert.encoder.layer.6.head.9: 1.520
bert.encoder.layer.6.head.10: 1.352
bert.encoder.layer.6.head.11: 1.266
bert.encoder.layer.6.attention.output.dense.weight: 0.157
bert.encoder.layer.6.attention.output.dense.bias: 0.239
bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.301
bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.569
bert.encoder.layer.6.intermediate.dense.weight: 0.196
bert.encoder.layer.6.intermediate.dense.bias: 0.357
bert.encoder.layer.6.output.dense.weight: 0.187
bert.encoder.layer.6.output.dense.bias: 0.326
bert.encoder.layer.6.output.LayerNorm.weight: 0.868
bert.encoder.layer.6.output.LayerNorm.bias: 0.286
bert.encoder.layer.7.head.0: 1.349
bert.encoder.layer.7.head.1: 1.329
bert.encoder.layer.7.head.2: 1.511
bert.encoder.layer.7.head.3: 1.659
bert.encoder.layer.7.head.4: 1.398
bert.encoder.layer.7.head.5: 1.434
bert.encoder.layer.7.head.6: 1.505
bert.encoder.layer.7.head.7: 1.656
bert.encoder.layer.7.head.8: 1.272
bert.encoder.layer.7.head.9: 1.315
bert.encoder.layer.7.head.10: 1.245
bert.encoder.layer.7.head.11: 1.418
bert.encoder.layer.7.attention.output.dense.weight: 0.158
bert.encoder.layer.7.attention.output.dense.bias: 0.248
bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.336
bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.572
bert.encoder.layer.7.intermediate.dense.weight: 0.192
bert.encoder.layer.7.intermediate.dense.bias: 0.350
bert.encoder.layer.7.output.dense.weight: 0.184
bert.encoder.layer.7.output.dense.bias: 0.357
bert.encoder.layer.7.output.LayerNorm.weight: 0.976
bert.encoder.layer.7.output.LayerNorm.bias: 0.285
bert.encoder.layer.8.head.0: 1.662
bert.encoder.layer.8.head.1: 1.591
bert.encoder.layer.8.head.2: 1.546
bert.encoder.layer.8.head.3: 1.539
bert.encoder.layer.8.head.4: 1.522
bert.encoder.layer.8.head.5: 1.792
bert.encoder.layer.8.head.6: 1.699
bert.encoder.layer.8.head.7: 1.461
bert.encoder.layer.8.head.8: 1.455
bert.encoder.layer.8.head.9: 1.451
bert.encoder.layer.8.head.10: 1.535
bert.encoder.layer.8.head.11: 1.400
bert.encoder.layer.8.attention.output.dense.weight: 0.166
bert.encoder.layer.8.attention.output.dense.bias: 0.267
bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.273
bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.532
bert.encoder.layer.8.intermediate.dense.weight: 0.193
bert.encoder.layer.8.intermediate.dense.bias: 0.346
bert.encoder.layer.8.output.dense.weight: 0.184
bert.encoder.layer.8.output.dense.bias: 0.379
bert.encoder.layer.8.output.LayerNorm.weight: 0.803
bert.encoder.layer.8.output.LayerNorm.bias: 0.284
bert.encoder.layer.9.head.0: 2.086
bert.encoder.layer.9.head.1: 1.900
bert.encoder.layer.9.head.2: 1.397
bert.encoder.layer.9.head.3: 1.676
bert.encoder.layer.9.head.4: 1.812
bert.encoder.layer.9.head.5: 1.341
bert.encoder.layer.9.head.6: 1.884
bert.encoder.layer.9.head.7: 1.929
bert.encoder.layer.9.head.8: 1.117
bert.encoder.layer.9.head.9: 1.891
bert.encoder.layer.9.head.10: 1.608
bert.encoder.layer.9.head.11: 1.333
bert.encoder.layer.9.attention.output.dense.weight: 0.167
bert.encoder.layer.9.attention.output.dense.bias: 0.299
bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.231
bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.544
bert.encoder.layer.9.intermediate.dense.weight: 0.192
bert.encoder.layer.9.intermediate.dense.bias: 0.335
bert.encoder.layer.9.output.dense.weight: 0.187
bert.encoder.layer.9.output.dense.bias: 0.383
bert.encoder.layer.9.output.LayerNorm.weight: 1.032
bert.encoder.layer.9.output.LayerNorm.bias: 0.255
bert.encoder.layer.10.head.0: 1.334
bert.encoder.layer.10.head.1: 2.848
bert.encoder.layer.10.head.2: 1.629
bert.encoder.layer.10.head.3: 1.262
bert.encoder.layer.10.head.4: 1.855
bert.encoder.layer.10.head.5: 2.184
bert.encoder.layer.10.head.6: 1.694
bert.encoder.layer.10.head.7: 1.434
bert.encoder.layer.10.head.8: 1.475
bert.encoder.layer.10.head.9: 2.371
bert.encoder.layer.10.head.10: 1.281
bert.encoder.layer.10.head.11: 1.651
bert.encoder.layer.10.attention.output.dense.weight: 0.170
bert.encoder.layer.10.attention.output.dense.bias: 0.347
bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.225
bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.462
bert.encoder.layer.10.intermediate.dense.weight: 0.189
bert.encoder.layer.10.intermediate.dense.bias: 0.303
bert.encoder.layer.10.output.dense.weight: 0.186
bert.encoder.layer.10.output.dense.bias: 0.426
bert.encoder.layer.10.output.LayerNorm.weight: 0.918
bert.encoder.layer.10.output.LayerNorm.bias: 0.276
bert.encoder.layer.11.head.0: 1.673
bert.encoder.layer.11.head.1: 1.655
bert.encoder.layer.11.head.2: 1.287
bert.encoder.layer.11.head.3: 1.820
bert.encoder.layer.11.head.4: 2.168
bert.encoder.layer.11.head.5: 2.080
bert.encoder.layer.11.head.6: 1.741
bert.encoder.layer.11.head.7: 1.904
bert.encoder.layer.11.head.8: 2.362
bert.encoder.layer.11.head.9: 1.567
bert.encoder.layer.11.head.10: 1.770
bert.encoder.layer.11.head.11: 1.964
bert.encoder.layer.11.attention.output.dense.weight: 0.181
bert.encoder.layer.11.attention.output.dense.bias: 0.354
bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.271
bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.410
bert.encoder.layer.11.intermediate.dense.weight: 0.189
bert.encoder.layer.11.intermediate.dense.bias: 0.284
bert.encoder.layer.11.output.dense.weight: 0.179
bert.encoder.layer.11.output.dense.bias: 0.271
bert.encoder.layer.11.output.LayerNorm.weight: 0.620
bert.encoder.layer.11.output.LayerNorm.bias: 0.337
bert.pooler.dense.weight: 0.353
bert.pooler.dense.bias: 0.347
qa_outputs.weight: 2.000
qa_outputs.bias: 2.000
Weights for encoder layers: {'bert.embeddings.word_embeddings.weight': tensor(0.7146, device='cuda:0'), 'bert.embeddings.position_embeddings.weight': tensor(0.2484, device='cuda:0'), 'bert.embeddings.token_type_embeddings.weight': tensor(0.1794, device='cuda:0'), 'bert.embeddings.LayerNorm.weight': tensor(1.7344, device='cuda:0'), 'bert.embeddings.LayerNorm.bias': tensor(0.8310, device='cuda:0'), 'bert.encoder.layer.0.head.0': tensor(3.0170, device='cuda:0'), 'bert.encoder.layer.0.head.1': tensor(1.7711, device='cuda:0'), 'bert.encoder.layer.0.head.2': tensor(2.3148, device='cuda:0'), 'bert.encoder.layer.0.head.3': tensor(3.5508, device='cuda:0'), 'bert.encoder.layer.0.head.4': tensor(1.6831, device='cuda:0'), 'bert.encoder.layer.0.head.5': tensor(3.9593, device='cuda:0'), 'bert.encoder.layer.0.head.6': tensor(1.9716, device='cuda:0'), 'bert.encoder.layer.0.head.7': tensor(1.7617, device='cuda:0'), 'bert.encoder.layer.0.head.8': tensor(1.6622, device='cuda:0'), 'bert.encoder.layer.0.head.9': tensor(1.9488, device='cuda:0'), 'bert.encoder.layer.0.head.10': tensor(2.3436, device='cuda:0'), 'bert.encoder.layer.0.head.11': tensor(2.1674, device='cuda:0'), 'bert.encoder.layer.0.attention.output.dense.weight': tensor(0.1411, device='cuda:0'), 'bert.encoder.layer.0.attention.output.dense.bias': tensor(0.2827, device='cuda:0'), 'bert.encoder.layer.0.attention.output.LayerNorm.weight': tensor(0.1727, device='cuda:0'), 'bert.encoder.layer.0.attention.output.LayerNorm.bias': tensor(1.2378, device='cuda:0'), 'bert.encoder.layer.0.intermediate.dense.weight': tensor(0.1851, device='cuda:0'), 'bert.encoder.layer.0.intermediate.dense.bias': tensor(0.2425, device='cuda:0'), 'bert.encoder.layer.0.output.dense.weight': tensor(0.1752, device='cuda:0'), 'bert.encoder.layer.0.output.dense.bias': tensor(0.3507, device='cuda:0'), 'bert.encoder.layer.0.output.LayerNorm.weight': tensor(0.6935, device='cuda:0'), 'bert.encoder.layer.0.output.LayerNorm.bias': tensor(0.3621, device='cuda:0'), 'bert.encoder.layer.1.head.0': tensor(2.3440, device='cuda:0'), 'bert.encoder.layer.1.head.1': tensor(1.6400, device='cuda:0'), 'bert.encoder.layer.1.head.2': tensor(0.9891, device='cuda:0'), 'bert.encoder.layer.1.head.3': tensor(1.0777, device='cuda:0'), 'bert.encoder.layer.1.head.4': tensor(1.6858, device='cuda:0'), 'bert.encoder.layer.1.head.5': tensor(1.1358, device='cuda:0'), 'bert.encoder.layer.1.head.6': tensor(1.4859, device='cuda:0'), 'bert.encoder.layer.1.head.7': tensor(2.0815, device='cuda:0'), 'bert.encoder.layer.1.head.8': tensor(1.2662, device='cuda:0'), 'bert.encoder.layer.1.head.9': tensor(1.9359, device='cuda:0'), 'bert.encoder.layer.1.head.10': tensor(1.6462, device='cuda:0'), 'bert.encoder.layer.1.head.11': tensor(1.1142, device='cuda:0'), 'bert.encoder.layer.1.attention.output.dense.weight': tensor(0.1385, device='cuda:0'), 'bert.encoder.layer.1.attention.output.dense.bias': tensor(0.3043, device='cuda:0'), 'bert.encoder.layer.1.attention.output.LayerNorm.weight': tensor(0.3874, device='cuda:0'), 'bert.encoder.layer.1.attention.output.LayerNorm.bias': tensor(0.7609, device='cuda:0'), 'bert.encoder.layer.1.intermediate.dense.weight': tensor(0.1946, device='cuda:0'), 'bert.encoder.layer.1.intermediate.dense.bias': tensor(0.2349, device='cuda:0'), 'bert.encoder.layer.1.output.dense.weight': tensor(0.1852, device='cuda:0'), 'bert.encoder.layer.1.output.dense.bias': tensor(0.3058, device='cuda:0'), 'bert.encoder.layer.1.output.LayerNorm.weight': tensor(0.4161, device='cuda:0'), 'bert.encoder.layer.1.output.LayerNorm.bias': tensor(0.4118, device='cuda:0'), 'bert.encoder.layer.2.head.0': tensor(1.9922, device='cuda:0'), 'bert.encoder.layer.2.head.1': tensor(1.3686, device='cuda:0'), 'bert.encoder.layer.2.head.2': tensor(1.4882, device='cuda:0'), 'bert.encoder.layer.2.head.3': tensor(1.4516, device='cuda:0'), 'bert.encoder.layer.2.head.4': tensor(1.4031, device='cuda:0'), 'bert.encoder.layer.2.head.5': tensor(1.0560, device='cuda:0'), 'bert.encoder.layer.2.head.6': tensor(1.4318, device='cuda:0'), 'bert.encoder.layer.2.head.7': tensor(0.9374, device='cuda:0'), 'bert.encoder.layer.2.head.8': tensor(1.1330, device='cuda:0'), 'bert.encoder.layer.2.head.9': tensor(2.1446, device='cuda:0'), 'bert.encoder.layer.2.head.10': tensor(1.1650, device='cuda:0'), 'bert.encoder.layer.2.head.11': tensor(1.4372, device='cuda:0'), 'bert.encoder.layer.2.attention.output.dense.weight': tensor(0.1355, device='cuda:0'), 'bert.encoder.layer.2.attention.output.dense.bias': tensor(0.3972, device='cuda:0'), 'bert.encoder.layer.2.attention.output.LayerNorm.weight': tensor(0.3981, device='cuda:0'), 'bert.encoder.layer.2.attention.output.LayerNorm.bias': tensor(0.7046, device='cuda:0'), 'bert.encoder.layer.2.intermediate.dense.weight': tensor(0.1967, device='cuda:0'), 'bert.encoder.layer.2.intermediate.dense.bias': tensor(0.2701, device='cuda:0'), 'bert.encoder.layer.2.output.dense.weight': tensor(0.1860, device='cuda:0'), 'bert.encoder.layer.2.output.dense.bias': tensor(0.3211, device='cuda:0'), 'bert.encoder.layer.2.output.LayerNorm.weight': tensor(0.3311, device='cuda:0'), 'bert.encoder.layer.2.output.LayerNorm.bias': tensor(0.3816, device='cuda:0'), 'bert.encoder.layer.3.head.0': tensor(1.6557, device='cuda:0'), 'bert.encoder.layer.3.head.1': tensor(1.0058, device='cuda:0'), 'bert.encoder.layer.3.head.2': tensor(1.3546, device='cuda:0'), 'bert.encoder.layer.3.head.3': tensor(1.0560, device='cuda:0'), 'bert.encoder.layer.3.head.4': tensor(1.1549, device='cuda:0'), 'bert.encoder.layer.3.head.5': tensor(1.3137, device='cuda:0'), 'bert.encoder.layer.3.head.6': tensor(1.2415, device='cuda:0'), 'bert.encoder.layer.3.head.7': tensor(1.9272, device='cuda:0'), 'bert.encoder.layer.3.head.8': tensor(1.1817, device='cuda:0'), 'bert.encoder.layer.3.head.9': tensor(1.3616, device='cuda:0'), 'bert.encoder.layer.3.head.10': tensor(1.3175, device='cuda:0'), 'bert.encoder.layer.3.head.11': tensor(1.0352, device='cuda:0'), 'bert.encoder.layer.3.attention.output.dense.weight': tensor(0.1485, device='cuda:0'), 'bert.encoder.layer.3.attention.output.dense.bias': tensor(0.2771, device='cuda:0'), 'bert.encoder.layer.3.attention.output.LayerNorm.weight': tensor(0.2905, device='cuda:0'), 'bert.encoder.layer.3.attention.output.LayerNorm.bias': tensor(0.7028, device='cuda:0'), 'bert.encoder.layer.3.intermediate.dense.weight': tensor(0.1989, device='cuda:0'), 'bert.encoder.layer.3.intermediate.dense.bias': tensor(0.2804, device='cuda:0'), 'bert.encoder.layer.3.output.dense.weight': tensor(0.1903, device='cuda:0'), 'bert.encoder.layer.3.output.dense.bias': tensor(0.3269, device='cuda:0'), 'bert.encoder.layer.3.output.LayerNorm.weight': tensor(0.5687, device='cuda:0'), 'bert.encoder.layer.3.output.LayerNorm.bias': tensor(0.3178, device='cuda:0'), 'bert.encoder.layer.4.head.0': tensor(1.2241, device='cuda:0'), 'bert.encoder.layer.4.head.1': tensor(1.0738, device='cuda:0'), 'bert.encoder.layer.4.head.2': tensor(1.7395, device='cuda:0'), 'bert.encoder.layer.4.head.3': tensor(1.5891, device='cuda:0'), 'bert.encoder.layer.4.head.4': tensor(1.2680, device='cuda:0'), 'bert.encoder.layer.4.head.5': tensor(1.2363, device='cuda:0'), 'bert.encoder.layer.4.head.6': tensor(1.3071, device='cuda:0'), 'bert.encoder.layer.4.head.7': tensor(1.6543, device='cuda:0'), 'bert.encoder.layer.4.head.8': tensor(1.1623, device='cuda:0'), 'bert.encoder.layer.4.head.9': tensor(0.9776, device='cuda:0'), 'bert.encoder.layer.4.head.10': tensor(1.0598, device='cuda:0'), 'bert.encoder.layer.4.head.11': tensor(1.5813, device='cuda:0'), 'bert.encoder.layer.4.attention.output.dense.weight': tensor(0.1591, device='cuda:0'), 'bert.encoder.layer.4.attention.output.dense.bias': tensor(0.2065, device='cuda:0'), 'bert.encoder.layer.4.attention.output.LayerNorm.weight': tensor(0.3257, device='cuda:0'), 'bert.encoder.layer.4.attention.output.LayerNorm.bias': tensor(0.6085, device='cuda:0'), 'bert.encoder.layer.4.intermediate.dense.weight': tensor(0.1980, device='cuda:0'), 'bert.encoder.layer.4.intermediate.dense.bias': tensor(0.3222, device='cuda:0'), 'bert.encoder.layer.4.output.dense.weight': tensor(0.1897, device='cuda:0'), 'bert.encoder.layer.4.output.dense.bias': tensor(0.3048, device='cuda:0'), 'bert.encoder.layer.4.output.LayerNorm.weight': tensor(0.5390, device='cuda:0'), 'bert.encoder.layer.4.output.LayerNorm.bias': tensor(0.2926, device='cuda:0'), 'bert.encoder.layer.5.head.0': tensor(1.2230, device='cuda:0'), 'bert.encoder.layer.5.head.1': tensor(1.7275, device='cuda:0'), 'bert.encoder.layer.5.head.2': tensor(1.2823, device='cuda:0'), 'bert.encoder.layer.5.head.3': tensor(1.1960, device='cuda:0'), 'bert.encoder.layer.5.head.4': tensor(1.0321, device='cuda:0'), 'bert.encoder.layer.5.head.5': tensor(1.1206, device='cuda:0'), 'bert.encoder.layer.5.head.6': tensor(1.3680, device='cuda:0'), 'bert.encoder.layer.5.head.7': tensor(1.3440, device='cuda:0'), 'bert.encoder.layer.5.head.8': tensor(1.3842, device='cuda:0'), 'bert.encoder.layer.5.head.9': tensor(1.3093, device='cuda:0'), 'bert.encoder.layer.5.head.10': tensor(1.1584, device='cuda:0'), 'bert.encoder.layer.5.head.11': tensor(1.0994, device='cuda:0'), 'bert.encoder.layer.5.attention.output.dense.weight': tensor(0.1590, device='cuda:0'), 'bert.encoder.layer.5.attention.output.dense.bias': tensor(0.2295, device='cuda:0'), 'bert.encoder.layer.5.attention.output.LayerNorm.weight': tensor(0.2894, device='cuda:0'), 'bert.encoder.layer.5.attention.output.LayerNorm.bias': tensor(0.5692, device='cuda:0'), 'bert.encoder.layer.5.intermediate.dense.weight': tensor(0.1959, device='cuda:0'), 'bert.encoder.layer.5.intermediate.dense.bias': tensor(0.3413, device='cuda:0'), 'bert.encoder.layer.5.output.dense.weight': tensor(0.1872, device='cuda:0'), 'bert.encoder.layer.5.output.dense.bias': tensor(0.2794, device='cuda:0'), 'bert.encoder.layer.5.output.LayerNorm.weight': tensor(0.8233, device='cuda:0'), 'bert.encoder.layer.5.output.LayerNorm.bias': tensor(0.2823, device='cuda:0'), 'bert.encoder.layer.6.head.0': tensor(1.5965, device='cuda:0'), 'bert.encoder.layer.6.head.1': tensor(1.0973, device='cuda:0'), 'bert.encoder.layer.6.head.2': tensor(1.3548, device='cuda:0'), 'bert.encoder.layer.6.head.3': tensor(1.5068, device='cuda:0'), 'bert.encoder.layer.6.head.4': tensor(1.4591, device='cuda:0'), 'bert.encoder.layer.6.head.5': tensor(1.2339, device='cuda:0'), 'bert.encoder.layer.6.head.6': tensor(1.3379, device='cuda:0'), 'bert.encoder.layer.6.head.7': tensor(1.3052, device='cuda:0'), 'bert.encoder.layer.6.head.8': tensor(1.4422, device='cuda:0'), 'bert.encoder.layer.6.head.9': tensor(1.5201, device='cuda:0'), 'bert.encoder.layer.6.head.10': tensor(1.3516, device='cuda:0'), 'bert.encoder.layer.6.head.11': tensor(1.2661, device='cuda:0'), 'bert.encoder.layer.6.attention.output.dense.weight': tensor(0.1566, device='cuda:0'), 'bert.encoder.layer.6.attention.output.dense.bias': tensor(0.2392, device='cuda:0'), 'bert.encoder.layer.6.attention.output.LayerNorm.weight': tensor(0.3009, device='cuda:0'), 'bert.encoder.layer.6.attention.output.LayerNorm.bias': tensor(0.5690, device='cuda:0'), 'bert.encoder.layer.6.intermediate.dense.weight': tensor(0.1964, device='cuda:0'), 'bert.encoder.layer.6.intermediate.dense.bias': tensor(0.3569, device='cuda:0'), 'bert.encoder.layer.6.output.dense.weight': tensor(0.1870, device='cuda:0'), 'bert.encoder.layer.6.output.dense.bias': tensor(0.3265, device='cuda:0'), 'bert.encoder.layer.6.output.LayerNorm.weight': tensor(0.8675, device='cuda:0'), 'bert.encoder.layer.6.output.LayerNorm.bias': tensor(0.2863, device='cuda:0'), 'bert.encoder.layer.7.head.0': tensor(1.3487, device='cuda:0'), 'bert.encoder.layer.7.head.1': tensor(1.3288, device='cuda:0'), 'bert.encoder.layer.7.head.2': tensor(1.5110, device='cuda:0'), 'bert.encoder.layer.7.head.3': tensor(1.6589, device='cuda:0'), 'bert.encoder.layer.7.head.4': tensor(1.3983, device='cuda:0'), 'bert.encoder.layer.7.head.5': tensor(1.4339, device='cuda:0'), 'bert.encoder.layer.7.head.6': tensor(1.5047, device='cuda:0'), 'bert.encoder.layer.7.head.7': tensor(1.6564, device='cuda:0'), 'bert.encoder.layer.7.head.8': tensor(1.2719, device='cuda:0'), 'bert.encoder.layer.7.head.9': tensor(1.3148, device='cuda:0'), 'bert.encoder.layer.7.head.10': tensor(1.2447, device='cuda:0'), 'bert.encoder.layer.7.head.11': tensor(1.4175, device='cuda:0'), 'bert.encoder.layer.7.attention.output.dense.weight': tensor(0.1584, device='cuda:0'), 'bert.encoder.layer.7.attention.output.dense.bias': tensor(0.2484, device='cuda:0'), 'bert.encoder.layer.7.attention.output.LayerNorm.weight': tensor(0.3359, device='cuda:0'), 'bert.encoder.layer.7.attention.output.LayerNorm.bias': tensor(0.5717, device='cuda:0'), 'bert.encoder.layer.7.intermediate.dense.weight': tensor(0.1923, device='cuda:0'), 'bert.encoder.layer.7.intermediate.dense.bias': tensor(0.3503, device='cuda:0'), 'bert.encoder.layer.7.output.dense.weight': tensor(0.1839, device='cuda:0'), 'bert.encoder.layer.7.output.dense.bias': tensor(0.3575, device='cuda:0'), 'bert.encoder.layer.7.output.LayerNorm.weight': tensor(0.9762, device='cuda:0'), 'bert.encoder.layer.7.output.LayerNorm.bias': tensor(0.2845, device='cuda:0'), 'bert.encoder.layer.8.head.0': tensor(1.6617, device='cuda:0'), 'bert.encoder.layer.8.head.1': tensor(1.5912, device='cuda:0'), 'bert.encoder.layer.8.head.2': tensor(1.5457, device='cuda:0'), 'bert.encoder.layer.8.head.3': tensor(1.5387, device='cuda:0'), 'bert.encoder.layer.8.head.4': tensor(1.5221, device='cuda:0'), 'bert.encoder.layer.8.head.5': tensor(1.7921, device='cuda:0'), 'bert.encoder.layer.8.head.6': tensor(1.6986, device='cuda:0'), 'bert.encoder.layer.8.head.7': tensor(1.4613, device='cuda:0'), 'bert.encoder.layer.8.head.8': tensor(1.4555, device='cuda:0'), 'bert.encoder.layer.8.head.9': tensor(1.4506, device='cuda:0'), 'bert.encoder.layer.8.head.10': tensor(1.5352, device='cuda:0'), 'bert.encoder.layer.8.head.11': tensor(1.4003, device='cuda:0'), 'bert.encoder.layer.8.attention.output.dense.weight': tensor(0.1657, device='cuda:0'), 'bert.encoder.layer.8.attention.output.dense.bias': tensor(0.2673, device='cuda:0'), 'bert.encoder.layer.8.attention.output.LayerNorm.weight': tensor(0.2727, device='cuda:0'), 'bert.encoder.layer.8.attention.output.LayerNorm.bias': tensor(0.5319, device='cuda:0'), 'bert.encoder.layer.8.intermediate.dense.weight': tensor(0.1927, device='cuda:0'), 'bert.encoder.layer.8.intermediate.dense.bias': tensor(0.3462, device='cuda:0'), 'bert.encoder.layer.8.output.dense.weight': tensor(0.1838, device='cuda:0'), 'bert.encoder.layer.8.output.dense.bias': tensor(0.3788, device='cuda:0'), 'bert.encoder.layer.8.output.LayerNorm.weight': tensor(0.8031, device='cuda:0'), 'bert.encoder.layer.8.output.LayerNorm.bias': tensor(0.2844, device='cuda:0'), 'bert.encoder.layer.9.head.0': tensor(2.0855, device='cuda:0'), 'bert.encoder.layer.9.head.1': tensor(1.9001, device='cuda:0'), 'bert.encoder.layer.9.head.2': tensor(1.3973, device='cuda:0'), 'bert.encoder.layer.9.head.3': tensor(1.6764, device='cuda:0'), 'bert.encoder.layer.9.head.4': tensor(1.8119, device='cuda:0'), 'bert.encoder.layer.9.head.5': tensor(1.3415, device='cuda:0'), 'bert.encoder.layer.9.head.6': tensor(1.8843, device='cuda:0'), 'bert.encoder.layer.9.head.7': tensor(1.9293, device='cuda:0'), 'bert.encoder.layer.9.head.8': tensor(1.1171, device='cuda:0'), 'bert.encoder.layer.9.head.9': tensor(1.8906, device='cuda:0'), 'bert.encoder.layer.9.head.10': tensor(1.6079, device='cuda:0'), 'bert.encoder.layer.9.head.11': tensor(1.3328, device='cuda:0'), 'bert.encoder.layer.9.attention.output.dense.weight': tensor(0.1671, device='cuda:0'), 'bert.encoder.layer.9.attention.output.dense.bias': tensor(0.2989, device='cuda:0'), 'bert.encoder.layer.9.attention.output.LayerNorm.weight': tensor(0.2307, device='cuda:0'), 'bert.encoder.layer.9.attention.output.LayerNorm.bias': tensor(0.5441, device='cuda:0'), 'bert.encoder.layer.9.intermediate.dense.weight': tensor(0.1923, device='cuda:0'), 'bert.encoder.layer.9.intermediate.dense.bias': tensor(0.3354, device='cuda:0'), 'bert.encoder.layer.9.output.dense.weight': tensor(0.1872, device='cuda:0'), 'bert.encoder.layer.9.output.dense.bias': tensor(0.3826, device='cuda:0'), 'bert.encoder.layer.9.output.LayerNorm.weight': tensor(1.0323, device='cuda:0'), 'bert.encoder.layer.9.output.LayerNorm.bias': tensor(0.2548, device='cuda:0'), 'bert.encoder.layer.10.head.0': tensor(1.3342, device='cuda:0'), 'bert.encoder.layer.10.head.1': tensor(2.8476, device='cuda:0'), 'bert.encoder.layer.10.head.2': tensor(1.6292, device='cuda:0'), 'bert.encoder.layer.10.head.3': tensor(1.2623, device='cuda:0'), 'bert.encoder.layer.10.head.4': tensor(1.8554, device='cuda:0'), 'bert.encoder.layer.10.head.5': tensor(2.1840, device='cuda:0'), 'bert.encoder.layer.10.head.6': tensor(1.6938, device='cuda:0'), 'bert.encoder.layer.10.head.7': tensor(1.4342, device='cuda:0'), 'bert.encoder.layer.10.head.8': tensor(1.4754, device='cuda:0'), 'bert.encoder.layer.10.head.9': tensor(2.3712, device='cuda:0'), 'bert.encoder.layer.10.head.10': tensor(1.2813, device='cuda:0'), 'bert.encoder.layer.10.head.11': tensor(1.6513, device='cuda:0'), 'bert.encoder.layer.10.attention.output.dense.weight': tensor(0.1700, device='cuda:0'), 'bert.encoder.layer.10.attention.output.dense.bias': tensor(0.3474, device='cuda:0'), 'bert.encoder.layer.10.attention.output.LayerNorm.weight': tensor(0.2252, device='cuda:0'), 'bert.encoder.layer.10.attention.output.LayerNorm.bias': tensor(0.4621, device='cuda:0'), 'bert.encoder.layer.10.intermediate.dense.weight': tensor(0.1893, device='cuda:0'), 'bert.encoder.layer.10.intermediate.dense.bias': tensor(0.3032, device='cuda:0'), 'bert.encoder.layer.10.output.dense.weight': tensor(0.1862, device='cuda:0'), 'bert.encoder.layer.10.output.dense.bias': tensor(0.4261, device='cuda:0'), 'bert.encoder.layer.10.output.LayerNorm.weight': tensor(0.9180, device='cuda:0'), 'bert.encoder.layer.10.output.LayerNorm.bias': tensor(0.2761, device='cuda:0'), 'bert.encoder.layer.11.head.0': tensor(1.6731, device='cuda:0'), 'bert.encoder.layer.11.head.1': tensor(1.6551, device='cuda:0'), 'bert.encoder.layer.11.head.2': tensor(1.2869, device='cuda:0'), 'bert.encoder.layer.11.head.3': tensor(1.8203, device='cuda:0'), 'bert.encoder.layer.11.head.4': tensor(2.1680, device='cuda:0'), 'bert.encoder.layer.11.head.5': tensor(2.0805, device='cuda:0'), 'bert.encoder.layer.11.head.6': tensor(1.7408, device='cuda:0'), 'bert.encoder.layer.11.head.7': tensor(1.9041, device='cuda:0'), 'bert.encoder.layer.11.head.8': tensor(2.3617, device='cuda:0'), 'bert.encoder.layer.11.head.9': tensor(1.5673, device='cuda:0'), 'bert.encoder.layer.11.head.10': tensor(1.7702, device='cuda:0'), 'bert.encoder.layer.11.head.11': tensor(1.9644, device='cuda:0'), 'bert.encoder.layer.11.attention.output.dense.weight': tensor(0.1809, device='cuda:0'), 'bert.encoder.layer.11.attention.output.dense.bias': tensor(0.3535, device='cuda:0'), 'bert.encoder.layer.11.attention.output.LayerNorm.weight': tensor(0.2713, device='cuda:0'), 'bert.encoder.layer.11.attention.output.LayerNorm.bias': tensor(0.4101, device='cuda:0'), 'bert.encoder.layer.11.intermediate.dense.weight': tensor(0.1892, device='cuda:0'), 'bert.encoder.layer.11.intermediate.dense.bias': tensor(0.2838, device='cuda:0'), 'bert.encoder.layer.11.output.dense.weight': tensor(0.1788, device='cuda:0'), 'bert.encoder.layer.11.output.dense.bias': tensor(0.2707, device='cuda:0'), 'bert.encoder.layer.11.output.LayerNorm.weight': tensor(0.6200, device='cuda:0'), 'bert.encoder.layer.11.output.LayerNorm.bias': tensor(0.3372, device='cuda:0'), 'bert.pooler.dense.weight': tensor(0.3525, device='cuda:0'), 'bert.pooler.dense.bias': tensor(0.3467, device='cuda:0'), 'qa_outputs.weight': tensor(2., device='cuda:0'), 'qa_outputs.bias': tensor(2., device='cuda:0')}
Epoch 1/3
Training:   0%|                    | 0/1384 [00:00<?, ?it/s, loss=6.07, lr=5e-5]/root/workdir/squad.py:186: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  train_loss_df = pd.concat([train_loss_df, pd.DataFrame([{
Epoch 1/3 - Average Loss: 1.4113
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.72it/s]
Validation Loss: 1.0668
Exact Match (EM): 67.68
F1 Score: 77.59
/root/workdir/squad.py:276: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  eval_loss_df = pd.concat([eval_loss_df, pd.DataFrame([{
Epoch 2/3
Epoch 2/3 - Average Loss: 0.8130
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.73it/s]
Validation Loss: 0.9910
Exact Match (EM): 69.19
F1 Score: 78.98
Epoch 3/3
Epoch 3/3 - Average Loss: 0.5828
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.75it/s]
Validation Loss: 1.0867
Exact Match (EM): 69.84
F1 Score: 79.39
Training complete!
Saving model...
Model saved to ./fine_tuned_bert_qa

Process finished with exit code 0
