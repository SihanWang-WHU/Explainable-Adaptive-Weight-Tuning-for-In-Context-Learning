/usr/bin/python3 /root/workdir/squad.py
Loading dataset...
Creating DataLoaders...
Initializing model...
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Starting training...
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
BertForQuestionAnswering(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)
Some weights of BertModel were not initialized from the model checkpoint at fine_tuned_bert_qa and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using zero-initialized layer-head weights for encoder layers.
Weights for encoder layers: {'bert.embeddings.word_embeddings.weight': tensor(1., device='cuda:0'), 'bert.embeddings.position_embeddings.weight': tensor(1., device='cuda:0'), 'bert.embeddings.token_type_embeddings.weight': tensor(1., device='cuda:0'), 'bert.embeddings.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.embeddings.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.0': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.1': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.2': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.3': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.4': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.5': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.6': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.7': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.8': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.9': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.10': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.head.11': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.0.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.head.0': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.1': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.2': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.3': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.4': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.5': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.6': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.7': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.8': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.9': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.10': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.head.11': tensor(2., device='cuda:0'), 'bert.encoder.layer.1.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.1.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.head.0': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.1': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.2': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.3': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.4': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.5': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.6': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.7': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.8': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.9': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.10': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.head.11': tensor(3., device='cuda:0'), 'bert.encoder.layer.2.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.2.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.head.0': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.1': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.2': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.3': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.4': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.5': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.6': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.7': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.8': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.9': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.10': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.head.11': tensor(4., device='cuda:0'), 'bert.encoder.layer.3.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.3.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.head.0': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.1': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.2': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.3': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.4': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.5': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.6': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.7': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.8': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.9': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.10': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.head.11': tensor(5., device='cuda:0'), 'bert.encoder.layer.4.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.4.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.head.0': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.1': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.2': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.3': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.4': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.5': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.6': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.7': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.8': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.9': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.10': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.head.11': tensor(6., device='cuda:0'), 'bert.encoder.layer.5.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.5.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.head.0': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.1': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.2': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.3': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.4': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.5': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.6': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.7': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.8': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.9': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.10': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.head.11': tensor(7., device='cuda:0'), 'bert.encoder.layer.6.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.6.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.head.0': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.1': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.2': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.3': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.4': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.5': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.6': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.7': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.8': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.9': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.10': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.head.11': tensor(8., device='cuda:0'), 'bert.encoder.layer.7.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.7.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.head.0': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.1': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.2': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.3': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.4': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.5': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.6': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.7': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.8': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.9': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.10': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.head.11': tensor(9., device='cuda:0'), 'bert.encoder.layer.8.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.8.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.head.0': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.1': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.2': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.3': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.4': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.5': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.6': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.7': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.8': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.9': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.10': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.head.11': tensor(10., device='cuda:0'), 'bert.encoder.layer.9.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.9.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.head.0': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.1': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.2': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.3': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.4': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.5': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.6': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.7': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.8': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.9': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.10': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.head.11': tensor(11., device='cuda:0'), 'bert.encoder.layer.10.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.10.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.head.0': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.1': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.2': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.3': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.4': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.5': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.6': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.7': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.8': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.9': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.10': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.head.11': tensor(12., device='cuda:0'), 'bert.encoder.layer.11.attention.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.attention.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.attention.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.attention.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.intermediate.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.intermediate.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.output.dense.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.output.dense.bias': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.output.LayerNorm.weight': tensor(1., device='cuda:0'), 'bert.encoder.layer.11.output.LayerNorm.bias': tensor(1., device='cuda:0'), 'bert.pooler.dense.weight': tensor(1., device='cuda:0'), 'bert.pooler.dense.bias': tensor(1., device='cuda:0'), 'qa_outputs.weight': tensor(1., device='cuda:0'), 'qa_outputs.bias': tensor(1., device='cuda:0')}
Epoch 1/3
Training:   0%|                    | 0/1384 [00:00<?, ?it/s, loss=6.05, lr=5e-5]/root/workdir/squad.py:186: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  train_loss_df = pd.concat([train_loss_df, pd.DataFrame([{
Epoch 1/3 - Average Loss: 1.4020
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.70it/s]
Validation Loss: 1.0519
Exact Match (EM): 68.20
F1 Score: 77.74
/root/workdir/squad.py:276: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  eval_loss_df = pd.concat([eval_loss_df, pd.DataFrame([{
Epoch 2/3
Epoch 2/3 - Average Loss: 0.8068
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.68it/s]
Validation Loss: 1.0063
Exact Match (EM): 69.29
F1 Score: 79.22
Epoch 3/3
Epoch 3/3 - Average Loss: 0.5772
Evaluating: 100%|█████████████████████████████| 337/337 [00:28<00:00, 11.71it/s]
Validation Loss: 1.0818
Exact Match (EM): 69.56
F1 Score: 79.25
Training complete!
Saving model...
Model saved to ./fine_tuned_bert_qa

Process finished with exit code 0
